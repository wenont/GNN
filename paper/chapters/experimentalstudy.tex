\chapter{Experimental study}\label{chap:experiments}
\section{Experimental setup}
The following section outlines the planned methods and materials to be employed in the experiment, including the training framework, the datasets, the models, and the graph parameters.

\section{Experimental procedures}\label{sec:Experimental procedures}
The Experimental procedures are as follows:

\begin{enumerate}
    \item Obtain the dataset following the setup described in Section~\ref{sec:datasets}.
    \item Perform hyperparameter optimization using Bayesian methods, as detailed in Section~\ref{sec:hyperparameters tuning}.
    \item Train the models with the optimized hyperparameters and evaluate their generalization errors.
    \item Concurrently, calculate the graph parameters for each dataset, as detailed in Section~\ref{sec:practical_graph_parameters}.
    \item Finally, evaluate the impact of the graph parameters on the generalization error of the models by calculating the correlation between the graph parameters and the generalization error.
\end{enumerate}


\subsection{Training framework}
The training framework is based on PyTorch Geometric~\cite{fey2019fast}. PyTorch Geometric is a library for deep learning on graphs and other irregular structures. It consists of various methods and utilities to ease the implementation of GNNs. 

\subsection{Datasets}\label{sec:datasets}
The dataset used for the classification is TUDataset~\cite{morris2020tudataset}. TUDataset is often used for the GNN evaluation. It consists of data from different domains, including small molecules, bioinformatics, social networks and synthetic. Since the size of some datasets is quite small (less than 500 data points/graphs), 10-fold cross-validation is implemented to maximize data utilization.  The dataset is divided into three parts in a 1:1:8 ratio, with one portion designated as the test set, another as the validation set, and the remaining larger portion used for training. The remaining folds are used for training. The training is repeated 10 times, each time with a different test/val fold. The average of the generalization error over the 10 runs is calculated as the final generalization error of the model.

For the dataset with large size, the random 4000 data points are selected. The remaining data points are discarded. The reason is that some datasets are too large to be trained in a reasonable amount of time.

General information of the 49 datasets utilized in the experiment are provided in Appendix~\ref{chap:basic-info}.

\subsection{Hyperparameters tuning}\label{sec:hyperparameters tuning}
Before training the models, for each pair of dataset and model, the hyperparameters of the model are tuned. In the experiment, 5-fold cross-validation is used to tune the hyperparameters instead of 10-fold cross validation in the final experiment. The reason is that the 10-fold cross-validation is computationally expensive and time-consuming. 

The hyperparameters were optimized using the Bayesian optimization method~\cite{frazier2018tutorial}, facilitated by the Weights and Biases (WandB) framework~\cite{wandb}, which enabled precise tracking of validation accuracy and corresponding hyperparameters throughout the tuning process. A total of 196 dataset-model pairs were evaluated, derived from the combination of four models and 49 datasets. For each pair, the optimization process systematically adjusted hyperparameters to maximize validation accuracy.

The hyperparameters that have been tuned include the learning rate, batch size, hidden dimension, number of epochs, patience in early stopping, number of hidden layers, type of normalization, and patience of plateau scheduler. Additionally, for the GATv2 model, the number of heads, the dropout rate, and residual are also tuned. Furthermore, for the MPNN model, the number of MLP layers is tuned.

\subsection{Models}\label{sec:models}
In the experiment, I used four different GNN layers in each model, including Graph Convolutional Networks(GCN)~\cite{kipf2016semi}, Graph Attention Networks(GAT)~\cite{velickovic2020pointer}, Graph Attention Networks v2(GATv2)~\cite{brody2021attentive}, Simplified Graph Convolution(SGC)~\cite{wu2019simplifying}, and Message Passing Neural Networks(MPNN)~\cite{gilmer2017neural}. 

\section{Graph parameters under investigation}\label{sec:practical_graph_parameters}
As graph parameters defined abstractly in the section~\ref{sec:theoretical_graph_parameters}, I give here the concrete definition of the graph parameters that are used in the experiment. The graph parameters are calculated for each dataset and used as input features for the model. The graph parameters are calculated using the NetworkX library~\cite{hagberg2008exploring}.

\begin{description}
    \item [Average degree] The average degree of the graph $d_{avg}$ is defined as the average of the degrees of all nodes in the graph, that is
    $$ d_{avg} = \frac{1}{|V|} \sum_{v \in V} d(v)$$
    \item [Average shortest path length] The average shortest path length of the graph $a$ is defined as the average of the shortest paths between all pairs of nodes in the graph. The shortest path $d(s, t)$ between two nodes $v$ and $w$ is the minimum number of edges that need to be traversed to go from $v$ to $w$. The formal definition is
    $$
a = \frac{1}{|V|(|V|-1)} \sum_{v \in V} \sum_{w \in V, w \neq v} d(v, w)
    $$

     \item [Graph diameter] The graph diameter $\delta$ is the maximum of the shortest paths between all pairs of nodes in the graph, that is,
    $$
        \delta = \max_{v \in V} \max_{w \in V, w \neq v} d(v, w)
    $$

    \item [Graph density] The graph density $p$ is the ratio of the number of edges in the graph to the number of the maximum possible edges in the graph, that is,
    $$
p = \frac{2|E|}{|V|(|V|-1)}
    $$
    
    \item [Graph clustering coefficient]~\cite{luce1949method} The graph clustering coefficient $C$ is a measure of the degree to which nodes in a graph tend to cluster together. The clustering coefficient of a node $v$ (as known as the local clustering coefficient) is defined as the fraction of the number of triangles that include node $v$ to the maximum possible number of triples centered on node $v$. The clustering coefficient of the graph (as known as the global clustering coefficient) is defined as the average of the clustering coefficients of all nodes in the graph. The formal definition is
    $$
C = \frac{1}{|V|} \sum_{v \in V} \frac{2\cdot |\{(i,j)\in E \mid i, j \in N(v)\}|}{d(v)(d(v)-1)}
    $$
    where the term $|\{(i,j)\in E \mid i, j \in N(v)\}|$ is the number of edges between the neighbors of node $v$, i.e. the number of triangles containing node $v$. Furthermore, the term $\sum_{v \in V} \frac{2\cdot |\{(i,j)\in E \mid i, j \in N(v)\}|}{d(v)(d(v)-1)}$ is the sum of the local clustering coefficients of all nodes in the graph, in which the degree of node $v$ is denoted as $d(v)$.

    \item [Centrality measure of graphs] At first we have the definition of centrality measure in the \textbf{node level}. The centrality measure of a node is a measure of the importance of the node in the graph. There are many centrality measures, such as degree centrality, closeness centrality, betweenness centrality, and eigenvector centrality. 
    
    \textit{Degree centrality} is identical to the average degree of the graph defined above. 
    
    \textit{Closeness centrality}~\cite{bavelas1950communication} $C_C(v)$ is defined as the reciprocal of the sum of the length of the shortest paths from the node $v$ to all other nodes in the graph, that is 
    $$C_C(v) = \frac{1}{\sum_{w\in V}{d(v ,w)}}$$
    where $d(v, w)$ is the shortest path between node $v$ and node $w$. 
    
    \textit{Betweenness centrality}~\cite{freeman1977set} $C_B(v)$ is the sum of the fraction of the shortest paths between all pairs of nodes that pass through node $v$, that is
    
    $$
        C_B(v) = \sum_{s,t \in V} \frac{\sigma_{st}(v)}{\sigma_{st}}
    $$
    where $\sigma_{st}$ is the number of the shortest paths between nodes $s$ and $t$, and $\sigma_{st}(v)$ is the number of the shortest paths between nodes $s$ and $t$ that pass through node $v$.
    
    \textit{Eigenvector centrality}~\cite{bonacich1972factoring}\cite{bonacich2007some} is a measure of the influence of a node in a graph based on the concept that connections to high-scoring nodes contribute more to the score of the node in question. Formally, the eigenvector centrality $C_E(v)$ of a node $v$  is defined as the $v$-th component of the eigenvector corresponding to the largest eigenvalue of the adjacency matrix $A$ of the graph. Mathematically, it can be expressed as:
    
    \[ C_E(v) = \frac{1}{\lambda} \sum_{u \in N(v)} a_{vu} C_E(u) \]
    
    where $ \lambda$ is the largest eigenvalue of the adjacency matrix $A$.
$a_{vu}$ is the element of the adjacency matrix $A$ corresponding to the edge between nodes $v$  and $u$.
$ C_E(u)$  is the eigenvector centrality of node $u$.


    Finally, in the \textbf{graph level}, the centrality measure of the graph is defined as the average of the centrality measures over all nodes in the graph.

    \item [Average number of coloring in the 1-WL algorithm] 
    \sloppy
    The 1-dimensional Weisfeiler-Leman algorithm (1-WL)~\cite{weisfeiler1968reduction} is an algorithm that assigns a unique label (color) to each node in the graph, also known as color refinement. The average number of coloring in the 1-WL algorithm is defined as the average number of colors used to color the nodes in the graph. The 1-WL algorithm is a powerful graph isomorphism algorithm that can be used to determine if two graphs are isomorphic for a broad class of graphs~\cite{babai1979canonical}.  
\end{description}