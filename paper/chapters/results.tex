\chapter{Results \& discussion}\label{chap:results}

In this chapter, the results of the experiments are presented and discussed. The experiments were conducted to evaluate the generalization performance of four GNN architectures on 49 graph classification datasets. The four neural network architectures are described in Chapter \ref{chap:experiments}. The datasets using for training and testing are described in Section \ref{sec:datasets}. The experiments were conducted using the training and testing procedures described in Section \ref{sec:Experimental procedures}.

The following sections present the results of the experiments. First, the graph parameters calculated for the datasets are shown in~\ref{sec:result_graph_parameters}. Then, the generalization error of the four GNN architectures is presented individually in~\ref{sec:result_gcn},~\ref{sec:result_sgc},~\ref{sec:result_gatv2}, and~\ref{sec:result_mpnn}. Finally, the correlation analysis between the generalization error and the graph parameters is discussed in each section.

It should be noted that the correlation analysis for each architecture is presented with two scatter plots. The first contains all datasets, while the second contains only datasets with more than 1000 graphs. The reason for filtering the datasets in this way is that the datasets with fewer graphs tend to have higher variance in generalization error, which may lead to misleading correlations.

\section{Graph parameters}~\label{sec:result_graph_parameters}

The graph parameters calculated in the experiments are shown in the Chapter~\ref{chap:graph-parameters}.

\section{Graph Convolutional Networks}~\label{sec:result_gcn}
The calculated generalization error of the experiments using Graph Convolutional Networks are shown in Table~\ref{tab:ge_gcn}.


\begin{table}[!ht]
    \centering
    \footnotesize
    \begin{tabular}{p{3.5cm}|p{5cm}p{5cm}}
    \hline
    \toprule
        Name & Ave. generalization error & Standard deviation \\ 
    \midrule
        AIDS & 0.0081874905154109 & 0.0117165874689817 \\ 
        BZR & 0.0708846226334571 & 0.0609733723104 \\ 
        BZR\_MD & 0.0961788743734359 & 0.1091229766607284 \\ 
        COLORS-3 & 0.06828124076128 & 0.0298492461442947 \\ 
        COX2 & 0.0840000063180923 & 0.1163316890597343 \\ 
        COX2\_MD & 0.0750205665826797 & 0.1105385944247245 \\ 
        DD & 0.0138164404779672 & 0.0569455623626709 \\ 
        DHFR & 0.1413267403841018 & 0.0572429783642292 \\ 
        DHFR\_MD & 0.0547252781689167 & 0.1071747243404388 \\ 
        ENZYMES & 0.1968750059604644 & 0.1035036742687225 \\ 
        ER\_MD & 0.0477907545864582 & 0.0669434815645217 \\ 
        FRANKENSTEIN & 0.0549374930560588 & 0.0326129123568534 \\ 
        KKI & 0.1822761297225952 & 0.3100946843624115 \\ 
        MCF-7 & 0.0019999979995191 & 0.0135001158341765 \\ 
        MCF-7H & 0.003406238509342 & 0.0127300517633557 \\ 
        MOLT-4 & 0.0015624940861016 & 0.0123682264238595 \\ 
        MOLT-4H & 0.0034062266349792 & 0.0167727246880531 \\ 
        MUTAG & 0.1002193242311477 & 0.0946124866604805 \\ 
        Mutagenicity & 0.040156252682209 & 0.0229900479316711 \\ 
        NCI-H23 & 0.0028750181663781 & 0.0120867453515529 \\ 
        NCI-H23H & 0.0005312621360644 & 0.0108473878353834 \\ 
        NCI1 & 0.0832187384366989 & 0.0319501645863056 \\ 
        NCI109 & 0.0711562559008598 & 0.0383276790380477 \\ 
        OHSU & 0.3857143521308899 & 0.2356772273778915 \\ 
        OVCAR-8 & 0.0038125098217278 & 0.0155245345085859 \\ 
        OVCAR-8H & 0.0028437494765967 & 0.0162141937762498 \\ 
        P388 & 0.0043125031515955 & 0.013600654900074 \\ 
        P388H & 0.0070000053383409 & 0.0142645183950662 \\ 
        PC-3 & 0.0024375021457672 & 0.0098565947264432 \\ 
        PC-3H & 0.0060312449932098 & 0.0117860734462738 \\ 
        PROTEINS\_full & 0.0026662945747375 & 0.0525967963039875 \\ 
        PTC\_FM & 0.0923068895936012 & 0.1087017878890037 \\ 
        PTC\_FR & 0.1018810272216796 & 0.1225867569446563 \\ 
        PTC\_MM & 0.088215485215187 & 0.0875236690044403 \\ 
        PTC\_MR & 0.0782608613371849 & 0.0882423818111419 \\ 
        Peking\_1 & 0.2760869860649109 & 0.2045421004295349 \\ 
        SF-295 & 0.0032500028610229 & 0.0157351642847061 \\ 
        SF-295H & 0.0017812550067901 & 0.0091287596151232 \\ 
        SN12C & 0.0003750085888896 & 0.0128499586135149 \\ 
        SN12CH & 0.0031250000465661 & 0.0179813411086797 \\ 
        SW-620 & 0.0014062583213672 & 0.0130064394325017 \\ 
        SW-620H & 0.0014687419170513 & 0.0133227882906794 \\ 
        SYNTHETIC & 0.0120833460241556 & 0.0160114392638206 \\ 
        SYNTHETICnew & 0.1366666555404663 & 0.1849549412727356 \\ 
        Synthie & 0.1234375014901161 & 0.0413857623934745 \\ 
        UACC257 & 0.0011249959934502 & 0.0108615579083561 \\ 
        UACC257H & 0.000843757414259 & 0.0126192783936858 \\ 
        Yeast & 0.0053749978542327 & 0.027795847505331 \\ 
        YeastH & 0.0001875102461781 & 0.0135009009391069 \\ 
    \bottomrule
    \end{tabular}
    \caption{Generalization error of the GCN model on 49 datasets}
    \label{tab:ge_gcn} % ge -> generalization error
\end{table}

The correlation between the generalization error and the graph parameters are shown in Figure~\ref{fig:correlation_GCN} and Figure~\ref{fig:correlation_ignore_less_than_1000_GCN}.

% -------------------------------------------------------------------------------------------------

\subsection{Discussion}

The experimental results presented with \ref{fig:correlation_GCN} and \ref{fig:correlation_ignore_less_than_1000_GCN}  above demonstrate the relationships between graph parameters and the average generalization error across datasets of varying sizes. It gives insights into how different graph parameters influence the generalization performance of Graph Convolutional Networks (GCNs) in graph classification tasks. The following observations are noted from the results:

\subsubsection{Correlation analysis}

\paragraph{Negative correlations with graph diameter and shortest path}
The results indicate a moderate negative correlation between generalization error and both graph diameter (-0.21) and average shortest path (-0.27) in the full dataset analysis~\ref{fig:correlation_GCN}. When considering only larger datasets~\ref{fig:correlation_ignore_less_than_1000_GCN}, these correlations become stronger (-0.33 and -0.36, respectively), suggesting that GNNs generalize better on graphs with shorter path lengths and smaller diameters. 

\paragraph{Influence of graph density and clustering coefficient}
The correlation with graph density is relatively weak (0.09 in the full dataset~\ref{fig:correlation_GCN}, 0.27 in the filtered dataset~\ref{fig:correlation_ignore_less_than_1000_GCN}), but the clustering coefficient shows a higher correlation of 0.29 in the full dataset, which drops to 0.04 in the filtered dataset. This suggests that clustering coefficient may influence generalization, but likely due to the higher noise levels in smaller samples, the effect diminishes when excluding small datasets. On the contrary, the graph density indicate a moderate positive correlation in the filtered dataset, which is not observed in the full dataset.


\paragraph{Centrality measures}
Closeness centrality shows a consistent positive correlation with generalization error (0.15 in the full dataset, 0.35 in the filtered dataset). On the other hand, betweenness centrality has a moderate negative correlation (-0.23 in the full dataset, -0.03 in the filtered dataset). Eigenvector Centrality shows weak correlations in both cases (-0.06 and 0.21). Overall, if the dataset size is considered, closeness centrality and eigenvector centrality appear to have a moderate positive influence on generalization error, while betweenness centrality has a negative effect.

\paragraph{1-WL color count}
The correlation with 1-WL color count is relatively weak (0.09 and 0.03 in full and filtered datasets, respectively). This finding is interesting, as it indicates that while the Weisfeiler-Lehman test is commonly used as a measure of GNN expressiveness, it does not necessarily dictate generalization behavior, for the GCN model at least.

\subsubsection{Summary}
Overall, structural features like average shortest path, graph diameter, and  centrality measure have stronger relationships with generalization error compared to measures like 1-WL color count and potentially clustering coefficient. Nonetheless, none of the parameters shows a highly significant correlation.
% -------------------------------------------------------------------------------------------------

\newpage

\section{Simplified Graph Convolution}~\label{sec:result_sgc}
The calculated generalization error of the experiments using Simplified Graph Convolution are shown in Table~\ref{tab:ge_sgc}.


\begin{table}[!ht]
    \centering
    \footnotesize
    \begin{tabular}{p{3.5cm}|p{5cm}p{5cm}}
    \hline
    \toprule
        Name & Ave. generalization error & Standard deviation \\ 
    \midrule
        AIDS & 0.0094999792054295 & 0.0067623262293636 \\
        BZR & 0.0398076958954334 & 0.0535244457423687 \\
        BZR\_MD & 0.1050406470894813 & 0.095137633383274 \\
        COLORS-3 & 0.0739687532186508 & 0.0297187566757202 \\
        COX2 & 0.0861333459615707 & 0.0670665428042411 \\
        COX2\_MD & 0.0440740659832954 & 0.1867186874151229 \\
        DD & 0.1082454919815063 & 0.0635362416505813 \\
        DHFR & 0.1035643592476844 & 0.0682980865240097 \\
        DHFR\_MD & 0.0555799715220928 & 0.1178753077983856 \\
        ENZYMES & 0.1837499886751175 & 0.0857001841068267 \\
        ER\_MD & 0.0475622005760669 & 0.0829107165336608 \\
        FRANKENSTEIN & 0.061875008046627 & 0.0329361073672771 \\
        KKI & 0.0291044823825359 & 0.2818436622619629 \\
        MCF-7 & 0.000656247138977 & 0.0137401102110743 \\
        MCF-7H & 0.001156234764494 & 0.0151891019195318 \\
        MOLT-4 & 0.0018124937778338 & 0.0116189513355493 \\
        MOLT-4H & 0.0018437386024743 & 0.0139454510062932 \\
        MUTAG & 0.1163011640310287 & 0.0678915232419967 \\
        Mutagenicity & 0.0481875017285347 & 0.0258940625935792 \\
        NCI-H23 & 0.0031250119209289 & 0.0100476266816258 \\
        NCI-H23H & 5.9604645663569045e-09 & 0.0108693270012736 \\
        NCI1 & 0.0839374884963035 & 0.0166628845036029 \\
        NCI109 & 0.0677187517285347 & 0.0309047140181064 \\
        OHSU & 0.0270329676568508 & 0.2689114809036255 \\
        OVCAR-8 & 0.0021875083912163 & 0.0154096204787492 \\
        OVCAR-8H & 0.0021250010468065 & 0.0172242671251297 \\
        P388 & 0.0042812586762011 & 0.0127577884122729 \\
        P388H & 0.0118749979883432 & 0.0183747168630361 \\
        PC-3 & -0.0001562476099934 & 0.0097229816019535 \\
        PC-3H & 0.000968748354353 & 0.0120369335636496 \\
        PROTEINS\_full & 0.0114296078681945 & 0.0617067217826843 \\
        PTC\_FM & 0.101800300180912 & 0.1584279388189315 \\
        PTC\_FR & 0.102684274315834 & 0.0604767650365829 \\
        PTC\_MM & 0.1027272716164588 & 0.1311521083116531 \\
        PTC\_MR & 0.1173060312867164 & 0.1128198280930519 \\
        Peking\_1 & 0.232065200805664 & 0.290688544511795 \\
        SF-295 & 0.0006874978425912 & 0.0154270688071846 \\
        SF-295H & 0.0021562576293945 & 0.0088528767228126 \\
        SN12C & 0.001562523888424 & 0.0126561466604471 \\
        SN12CH & -6.249547004699707e-05 & 0.0126207722350955 \\
        SW-620 & 0.0008437514188699 & 0.0120212435722351 \\
        SW-620H & 0.0024687468539923 & 0.0140694072470068 \\
        SYNTHETIC & 0.0154166761785745 & 0.0234100967645645 \\
        SYNTHETICnew & 0.1204166635870933 & 0.0984339192509651 \\
        Synthie & 0.1784375011920929 & 0.0635749474167823 \\
        UACC257 & 3.124475551885553e-05 & 0.0105321435257792 \\
        UACC257H & 5.9604645663569045e-09 & 0.0123304584994912 \\
        Yeast & 0.0027187527157366 & 0.0283915027976036 \\
        YeastH & 0.0021875023376196 & 0.0149637833237648 \\
    \bottomrule
    \end{tabular}
    \caption{Generalization error of the SGC model on 49 datasets}
    \label{tab:ge_sgc} % ge -> generalization error
\end{table}

The correlation between the generalization error and the graph parameters are shown in Figure~\ref{fig:correlation_SGC} and Figure~\ref{fig:correlation_ignore_less_than_1000_SGC}.


\subsection{Discussion}

The experimental results presented with \ref{fig:correlation_SGC} and \ref{fig:correlation_ignore_less_than_1000_SGC}  above demonstrate the relationships between graph parameters and the average generalization error across datasets of varying sizes. It gives insights into how different graph parameters influence the generalization performance of Simplified Graph Convolution (SGC) in graph classification tasks. The following observations are noted from the results:

\subsubsection{Correlation analysis}

\paragraph{Graph average degree}
The average degree shows a weak positive correlation with generalization error in the full dataset (0.13), which increases substantially to a strong correlation (0.56) when only datasets with more than 1000 graphs are considered. This implies that generalization error is more sensitive to average degree in larger graph datasets. The weak correlation in the full dataset may be due to noise introduced by smaller datasets.

\paragraph{Graph diameter and shortest path}
Both graph diameter (-0.31) and average shortest path (-0.34) exhibit moderate negative correlations with generalization error in the full dataset, indicating that datasets with more compact graph structures (smaller diameters and shorter path lengths) tend to generalize better. Interestingly, these correlations diminish in the filtered dataset to 0.05 and -0.00, respectively. It suggests that the significance of these measures decreases due to larger size of datasets. 

\paragraph{Graph density and clustering coefficient}
The clustering coefficient shows a moderate positive correlation with generalization error in both datasets (0.26 in the full dataset and 0.44 in the filtered dataset). This indicates that generalization error increases in denser neighborhoods, particularly in larger datasets. Graph density, on the other hand, shows weak correlations in both cases (0.14 in the full dataset and 0.12 in the filtered dataset). It suggests a weaker effect on generalization behavior compared to clustering coefficient.

\paragraph{Centrality measures}
Among the centrality measures, betweenness centrality exhibits negative correlations with generalization error (-0.22 in the full dataset and -0.36 in the filtered dataset). This indicates that higher betweenness centrality leads to lower generalization error, particularly in larger datasets. Closeness centrality and eigenvector centrality exhibit weak to moderate correlations (0.20 and 0.07 in the full dataset; 0.17 and -0.12 in the filtered dataset), comparing to the betweenness centrality. Overall, centrality measures have a weak to moderate influence on generalization error, with betweenness centrality showing the most significant effect.

\paragraph{1-WL color count}
The 1-WL color count demonstrates a weak positive correlation with generalization error in the full dataset (0.20), which becomes substantially stronger in the filtered dataset (0.58). It suggests that the expressiveness of the graph structure, as measured by 1-WL color count, plays a more significant role in generalization behavior in larger datasets.

\subsubsection{Summary}
The results indicate that structural features like average degree and clustering coefficient are more predictive of generalization behavior in larger datasets, as reflected by their stronger correlations in the filtered dataset. In contrast, graph parameters like average graph diameter and shortest path, which show moderate correlations in the full dataset, diminish as the dataset size increases. Centrality measures, particularly betweenness centrality, exhibit a more consistent relationship with generalization error, with stronger correlations in the filtered dataset. Lastly, 1-WL coloring number shows the most significant correlation with generalization error, indicating that the expressiveness of the graph structure plays an important role in generalization ability.

\section{Graph Attention Networks v2}~\label{sec:result_gatv2}
The calculated generalization error of the experiments using Graph Convolutional Networks are shown in Table~\ref{tab:ge_gatv2}.

\begin{table}[!ht]
    \centering
    \footnotesize
    \begin{tabular}{p{3.5cm}|p{5cm}p{5cm}}
    \hline
    \toprule
        Name & Ave. generalization error & Standard deviation \\ 
    \midrule
    AIDS & 0.0043124915100634 & 0.0093050524592399 \\ 
    BZR & 0.0609999969601631 & 0.0820248797535896 \\ 
    BZR\_MD & 0.0472357682883739 & 0.1092826277017593 \\ 
    COLORS-3 & 0.043281253427267 & 0.0240429937839508 \\ 
    COX2 & 0.0630377084016799 & 0.0825407281517982 \\ 
    COX2\_MD & 0.085967056453228 & 0.0940771996974945 \\ 
    DD & 0.1108267903327941 & 0.0513695515692234 \\ 
    DHFR & 0.1174587234854698 & 0.0517340935766696 \\ 
    DHFR\_MD & 0.0339438319206237 & 0.1088754534721374 \\ 
    ENZYMES & 0.2141666859388351 & 0.0753771364688873 \\ 
    ER\_MD & 0.0706957802176475 & 0.0536176972091198 \\ 
    FRANKENSTEIN & 0.0540624968707561 & 0.0219314694404602 \\ 
    KKI & 0.2389925420284271 & 0.3365208804607391 \\ 
    MCF-7H & 0.0001249790220754 & 0.0124741122126579 \\ 
    MCF-7 & 0.0007500052452087 & 0.0141399931162595 \\ 
    MOLT-4H & 0.0006562412017956 & 0.0142586100846529 \\ 
    MOLT-4 & 0.0004374861600808 & 0.0111289750784635 \\ 
    MUTAG & 0.0780701860785484 & 0.0887901559472084 \\ 
    Mutagenicity & 0.0441875047981739 & 0.0323983952403068 \\ 
    NCI-H23H & 5.9604645663569045e-09 & 0.0108693270012736 \\ 
    NCI-H23 & 9.377002425026149e-05 & 0.0100130606442689 \\ 
    NCI109 & 0.087562508881092 & 0.0293439049273729 \\ 
    NCI1 & 0.079031229019165 & 0.0203865990042686 \\ 
    OHSU & 0.2868131995201111 & 0.3649870157241821 \\ 
    OVCAR-8H & 6.25073880655691e-05 & 0.0174949076026678 \\ 
    OVCAR-8 & 0.0003125071525573 & 0.0130370762199163 \\ 
    P388H & 0.0005312621360644 & 0.0131217204034328 \\ 
    P388 & 0.0042187571525573 & 0.0145163964480161 \\ 
    PC-3H & -5.9604645663569045e-09 & 0.0116321872919797 \\ 
    PC-3 & 0.0003437519189901 & 0.0097120413556694 \\ 
    PROTEINS\_full & 0.0294719096273183 & 0.0491058491170406 \\ 
    PTC\_FM & 0.0593364052474498 & 0.0939571186900138 \\ 
    PTC\_FR & 0.0619827024638652 & 0.0776680409908294 \\ 
    PTC\_MM & 0.0872390493750572 & 0.1021735519170761 \\ 
    PTC\_MR & 0.0601662322878837 & 0.0595390647649765 \\ 
    Peking\_1 & 0.250543475151062 & 0.2304597347974777 \\ 
    SF-295H & 5.9604645663569045e-09 & 0.0102338008582592 \\ 
    SF-295 & 0.0002187669306294 & 0.0146516626700758 \\ 
    SN12CH & 0.0 & 0.0145929940044879 \\ 
    SN12C & 0.0006562650087289 & 0.0130099598318338 \\ 
    SW-620H & 0.0002187490463256 & 0.0138334389775991 \\ 
    SW-620 & 0.0002812623861245 & 0.0136981671676039 \\ 
    SYNTHETIC & 0.0212500095367431 & 0.0290307682007551 \\ 
    SYNTHETICnew & 0.1174999922513961 & 0.0795870795845985 \\ 
    Synthie & 0.1574999988079071 & 0.1155846491456031 \\ 
    UACC257H & 5.9604645663569045e-09 & 0.0123304584994912 \\ 
    UACC257 & 0.0005000054952688 & 0.0106954481452703 \\ 
    YeastH & 0.0011562525760382 & 0.0136780375614762 \\ 
    Yeast & 0.0016562461387366 & 0.0281631872057914 \\ 
    \bottomrule
    \end{tabular}
    \caption{Generalization error of the GATv2 model on 49 datasets}
    \label{tab:ge_gatv2} % ge -> generalization error
\end{table}


The correlation between the generalization error and the graph parameters are shown in Figure~\ref{fig:correlation_GATv2} and Figure~\ref{fig:correlation_ignore_less_than_1000_GATv2}.


\subsection{Discussion}

The experimental results presented with \ref{fig:correlation_GATv2} and \ref{fig:correlation_ignore_less_than_1000_GATv2} above demonstrate the relationships between graph parameters and the average generalization error across datasets of varying sizes. It gives insights into how different graph parameters influence the generalization performance of Graph Attention Networks v2 (GATv2) in graph classification tasks. The following observations are noted from the results:

\subsubsection{Correlation analysis}

\paragraph{Average degree}
A weak positive correlation of 0.12 between average degree and generalization error is observed in the full dataset analysis (\ref{fig:correlation_GATv2}). This correlation becomes significantly stronger (0.60) in the larger datasets (\ref{fig:correlation_ignore_less_than_1000_GATv2}), indicating that higher average degree may negatively impact generalization. This trend suggests that increased connectivity might lead to reduce the model's ability to effectively generalize to unseen data.

\paragraph{Shortest path and graph diameter}
In the full dataset, a negative correlation is noted between generalization error and both shortest path (-0.22) and graph diameter (-0.16), indicating that smaller and more compact graphs are associated with better generalization. However, in larger datasets, the negative correlation with the shortest path diminishes (0.11), while graph diameter shows a weak positive correlation (0.18). My interpretation is that the noise in smaller datasets may have influenced the initial negative correlations.

\paragraph{Graph density and clustering coefficient}
Graph density shows a very weak correlation in the full dataset (0.07) and remains weak in larger datasets (0.09). On the other hand, the clustering coefficient demonstrates considerable variability, with a strong positive correlation of 0.35 in the full dataset, which increases further (0.51) in the filtered dataset. This suggests the clustering coefficient has more impact on generalization than graph density, particularly in larger datasets.

\paragraph{Centrality measures}
Betweenness centrality exhibits a moderate negative correlation (-0.29) in the full dataset and has similar correlation in the filtered dataset (-0.31). The average eigenvector centrality has also a negative correlation (-0.16 both in the full dataset and the filtered dataset), but relatively weaker than betweenness centrality. Closeness centrality, however, has a weak positive correlation (0.12 in the full dataset, 0.09 in the filtered dataset), indicating little impact on generalization error. It is the weakest among the centrality measures. 

\paragraph{1-WL color count}
The 1-WL color count demonstrates moderate positive correlation in the full dataset (0.26), but this correlation increases significantly in larger datasets (0.59). It is similar to the results observed in the SGC model, suggesting that the expressiveness of the graph structure plays a more significant role in generalization behavior in larger datasets.

\subsubsection{Summary}
The results indicate that structural features like average degree and clustering coefficient are more predictive of generalization behavior in larger datasets, as reflected by their stronger correlations in the filtered dataset. In contrast, graph parameters like average graph diameter and shortest path, which show moderate negative correlations in the full dataset, but it becomes weak positive correlations in the filtered dataset. One possible explanation is that the noise in smaller datasets may have influenced the initial negative correlations. Centrality measures, particularly betweenness centrality, exhibit a more consistent relationship with generalization error, with stronger correlations in the filtered dataset. Lastly, 1-WL coloring number shows the most significant correlation with generalization error, indicating that the expressiveness of the graph structure plays an important role in generalization ability.

\section{Message Passing Neural Networks}~\label{sec:result_mpnn}
The calculated generalization error of the experiments using Message Passing Neural Networks are shown in Table~\ref{tab:ge_mpnn}.


\begin{table}[!ht]
    \centering
    \footnotesize
    \begin{tabular}{p{3.5cm}|p{5cm}p{5cm}}
    \hline
    \toprule
        Name & Ave. generalization error & Standard deviation \\ 
    \midrule
    AIDS & 0.0091874841600656 & 0.0070713576860725 \\ 
    BZR\_MD & 0.1059349551796913 & 0.141946479678154 \\ 
    BZR & 0.063807688653469 & 0.0739696696400642 \\ 
    COLORS-3 & 0.076031245291233 & 0.0253504272550344 \\ 
    COX2\_MD & 0.1141152158379554 & 0.0899139121174812 \\ 
    COX2 & 0.0715246349573135 & 0.0796237140893936 \\ 
    DD & 0.0929188206791877 & 0.0893803909420967 \\ 
    DHFR\_MD & 0.0675946250557899 & 0.1037261709570884 \\ 
    DHFR & 0.1216765642166137 & 0.063124693930149 \\ 
    ENZYMES & 0.1668750047683715 & 0.1106084510684013 \\ 
    ER\_MD & 0.0358938574790954 & 0.0568079650402069 \\ 
    FRANKENSTEIN & 0.0718437507748603 & 0.0375203862786293 \\ 
    KKI & 0.1231343299150466 & 0.2515161037445068 \\ 
    MCF-7H & 0.0091874776408076 & 0.0150008797645568 \\ 
    MCF-7 & 0.0080625060945749 & 0.0152003578841686 \\ 
    MOLT-4H & -0.0004062592925038 & 0.0148166185244917 \\ 
    MOLT-4 & 0.0010624944698065 & 0.0125815030187368 \\ 
    MUTAG & 0.0487573221325874 & 0.0706913545727729 \\ 
    Mutagenicity & 0.0664687603712081 & 0.0282005369663238 \\ 
    NCI-H23H & 0.0011562525760382 & 0.0115329325199127 \\ 
    NCI-H23 & 0.0021250187419354 & 0.0086612496525049 \\ 
    NCI109 & 0.0788125097751617 & 0.0241683777421712 \\ 
    NCI1 & 0.0700937509536743 & 0.0207409560680389 \\ 
    OHSU & 0.1410989016294479 & 0.2492884248495102 \\ 
    OVCAR-8H & 0.0036250054836273 & 0.0197824537754058 \\ 
    OVCAR-8 & 0.0032187581527978 & 0.0174466092139482 \\ 
    P388H & 0.0135312620550394 & 0.0137673774734139 \\ 
    P388 & 0.0091250063851475 & 0.0158420968800783 \\ 
    PC-3H & 0.0118124904111027 & 0.0230077672749757 \\ 
    PC-3 & 0.0024374960921704 & 0.0094347447156906 \\ 
    PROTEINS\_full & 0.0246215872466564 & 0.0341414250433445 \\ 
    PTC\_FM & 0.0814946442842483 & 0.1237905248999595 \\ 
    PTC\_FR & 0.0976715683937072 & 0.104591965675354 \\ 
    PTC\_MM & 0.1219191774725914 & 0.0936376005411148 \\ 
    PTC\_MR & 0.085997425019741 & 0.1167371049523353 \\ 
    Peking\_1 & 0.1918478459119796 & 0.1804585009813308 \\ 
    SF-295H & 0.0026875077746808 & 0.0119734490290284 \\ 
    SF-295 & 0.0074062468484044 & 0.0222921669483184 \\ 
    SN12CH & 0.0016874969005584 & 0.0161193944513797 \\ 
    SN12C & -0.0001249790220754 & 0.0123165436089038 \\ 
    SW-620H & 0.0044999956153333 & 0.0196564402431249 \\ 
    SW-620 & 0.0040312530472874 & 0.0153152821585536 \\ 
    SYNTHETIC & 0.0133333448320627 & 0.0172132737934589 \\ 
    SYNTHETICnew & 0.1375000178813934 & 0.0440958477556705 \\ 
    Synthie & 0.0699999928474426 & 0.0399163626134395 \\ 
    UACC257H & 0.0002187550126109 & 0.0126776117831468 \\ 
    UACC257 & -5.9604645663569045e-09 & 0.0111375767737627 \\ 
    YeastH & 0.0019687532912939 & 0.0153990918770432 \\ 
    Yeast & 0.0017500042449682 & 0.0269955322146415 \\ 
    \bottomrule
    \end{tabular}
    \caption{Generalization error of the MPNN model on 49 datasets}
    \label{tab:ge_mpnn} % ge -> generalization error
\end{table}

The correlation between the generalization error and the graph parameters are shown in Figure~\ref{fig:correlation_MPNN} and Figure~\ref{fig:correlation_ignore_less_than_1000_MPNN}.

\subsection{Discussion}

The experimental results illustrated in Figures \ref{fig:correlation_MPNN} and \ref{fig:correlation_ignore_less_than_1000_MPNN} explore the relationship between various graph structural parameters and the average generalization error across datasets of varied sizes. The analysis distinguishes between the full dataset (including datasets with fewer than 1000 graphs) and the filtered dataset (only datasets with more than 1000 graphs) to minimize the impact of high variability in smaller datasets. The key findings are detailed below.

\subsubsection{Correlation analysis}

\paragraph{Average degree}
In the full dataset, the average degree exhibits a moderate positive correlation (0.25) with generalization error, indicating a trend where higher average connectivity lowers generalization. However, this relationship strengthens significantly in larger datasets (correlation of 0.51), suggesting that average degree plays a more pronounced role in influencing generalization in datasets with robust sample sizes.

\paragraph{Shortest path and graph diameter}
The shortest path and graph diameter both exhibit moderate negative correlations (-0.36 and -0.32, respectively) with generalization error in the full dataset. This result suggests that more compact graph structures are generally favorable for generalization. However, these relationships weaken substantially in the filtered dataset, with correlations of -0.09 and -0.04, respectively. This divergence highlights the potential influence of noise in smaller datasets, reducing the reliability of these correlations when analyzing subsets of smaller sample sizes.

\paragraph{Graph density and clustering coefficient}
Graph density demonstrates a mild positive correlation (0.24) in the full dataset but substantially weakens (0.19) in the filtered dataset. Conversely, the clustering coefficient stands out with a more substantial positive correlation (0.41) in the full dataset, which remains consistent (0.43) in the filtered dataset. These findings underscore the significant impact of the clustering coefficient on generalization. It suggests the local grouping properties of a graph play a larger role in influencing model behavior across datasets of various sizes.

\paragraph{Centrality measures}
Betweenness centrality shows a consistent negative correlation (-0.29 in the full dataset and -0.32 in the filtered dataset). Conversely, closeness centrality exhibits a weak but positive correlation (0.29 in the full dataset and 0.25 in the filtered dataset), suggesting mild relevance to generalization. Finally, eigenvector centrality has minimal impact, with very weak correlations (0.06 in the full dataset and -0.05 in the filtered dataset), implying it is a less influential factor for generalization performance.

\paragraph{1-WL color count}
The 1-WL color count displays a moderate positive correlation (0.17) with generalization error in the full dataset, which grows stronger (0.49) in the filtered dataset. This clear trend across larger datasets emphasizes the influence of graph structure expressiveness as captured by the 1-WL test, suggesting that more diverse graph features challenge generalization potential.

\subsubsection{Summary}
The results highlight notable variations in the influence of graph parameters across full and filtered datasets. Average degree and the clustering coefficient emerge as consistently impactful structural features, with relatively stronger correlations observed in larger datasets. In contrast, parameters such as shortest path and graph diameter exhibit diminishing correlations in filtered datasets, likely due to noise in smaller samples. Notably, 1-WL color count reinforces the significance of graph expressiveness in generalization.

\newpage

\section{Limitations}~\label{sec:limitations}

\paragraph{Time-consuming hyperparameter tuning}
To conduct the experiments, I used a high-performance computing cluster with a NVIDIA A10 for the GCN, SGC, and GATv2 models, and a NVIDIA L40S for the MPNN model (due to its high memory requirements). While the computational resources were sufficient, the primary bottleneck arose from the lengthy hyperparameter tuning process. For each model, optimizing hyperparameters across all 49 datasets required approximately 2-3 weeks. This time-consuming process limited the scalability of the study to a larger number of models and datasets.

\paragraph{Noise in smaller datasets}
Some datesets have a small number of graphs, which introduce noise and reduce the reliability of the correlation analysis. To address this issue, I have filtered out datasets with fewer than 1000 graphs in the correlation analysis. However, this approach may have overlooked datasets with potentially valuable insights. 

\paragraph{High concentration of generalization error values}
The generalization error values for all models are highly concentrated, with approximately half falling within a 2\% range. Specifically, 26 out of 49 values for the GCN model, 25 out of 49 for the SGC model, 25 out of 49 for the GATv2 model, and 24 out of 49 for the MPNN model exhibit this level of concentration. This may be the result that there is no significantly strong correlation between the graph parameters and the generalization error for the models and datasets used in this study.

\paragraph{Limited model selection}
The study focuses on four popular graph neural network models (GCN, SGC, GATv2, and MPNN) and may not capture the full spectrum of graph neural network architectures. The results may not be generalizable to other models, and further research is needed to explore the relationship between graph parameters and generalization error across a broader range of models.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_GCN.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Graph Convolutional Networks (GCNs). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Colors indicate dataset sizes: blue (<1000 graphs), green (1000-4000 graphs), and red (>4000 graphs).}
    \label{fig:correlation_GCN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_ignore_less_than_1000_GCN.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Graph Convolutional Networks (GCNs). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Only datasets with more than 1000 graphs are considered. There are 30 datasets in this plot and another 19 datasets are not included.}
    \label{fig:correlation_ignore_less_than_1000_GCN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_SGC.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of a Simplified Graph Convolution (SGC) model. Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Colors indicate dataset sizes: blue (<1000 graphs), green (1000-4000 graphs), and red (>4000 graphs).}
    \label{fig:correlation_SGC}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_ignore_less_than_1000_SGC.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of a Simplified Graph Convolution (SGC) model. Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Only datasets with more than 1000 graphs are considered. There are 30 datasets in this plot and another 19 datasets are not included.}
    \label{fig:correlation_ignore_less_than_1000_SGC}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_GATv2.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Graph Attention Networks v2 (GATv2). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Colors indicate dataset sizes: blue (<1000 graphs), green (1000-4000 graphs), and red (>4000 graphs).}
    \label{fig:correlation_GATv2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_ignore_less_than_1000_GATv2.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Graph Attention Networks v2 (GATv2). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Only datasets with more than 1000 graphs are considered. There are 30 datasets in this plot and another 19 datasets are not included.}
    \label{fig:correlation_ignore_less_than_1000_GATv2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_MPNN.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Message Passing Neural Networks (MPNN). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Colors indicate dataset sizes: blue (<1000 graphs), green (1000-4000 graphs), and red (>4000 graphs).}
    \label{fig:correlation_MPNN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/correlation_ignore_less_than_1000_MPNN.png}
    \caption{Scatter plots showing the relationship between various graph parameters and the average generalization error of Message Passing Neural Networks (MPNN). Each subplot presents the correlation coefficient between a specific graph parameter and the generalization error. Only datasets with more than 1000 graphs are considered. There are 30 datasets in this plot and another 19 datasets are not included.}
    \label{fig:correlation_ignore_less_than_1000_MPNN}
\end{figure}
