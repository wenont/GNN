\chapter{Introduction}\label{chap:introduction}
Graph representation learning has emerged as a prominent area within machine learning, driven by its capacity to model relationships in graph-structured data effectively. Graph Neural Networks (GNNs)~\cite{gori2005new}, also known as deep learning on graphs, are a class of neural networks that can operate on graph-structured data, such as social networks~\cite{easley2010networks}, biological networks~\cite{barabasi2004network}, and images~\cite{simonovsky2017dynamic}. GNNs use the structure of a graph to perform ``message passing'', allowing information to propagate across nodes and edges. The key idea is to iteratively aggregate and update node features by exchanging information with local neighboring nodes, which allows GNNs to capture the structural information of the graph. This information is then used to make predictions. GNNs have been demonstrated to be effective in a wide range of tasks, such as node classification~\cite{hamilton2017inductive}, edge prediction~\cite{kipf2016variational}, and graph classification~\cite{gilmer2017neural}. Popular GNNs in recent years include Graph Convolutional Network (GCN)~\cite{kipf2016semi}, Graph Attention Network (GAT)~\cite{velickovic2020pointer}, and Message Passing Neural Network (MPNN)~\cite{gilmer2017neural}.  

From the perspective of \cite{morris2024futuredirectionstheorygraph}, the behavior of GNNs remains insufficiently understood, particularly with respect to how graph parameters/characteristics influence their empirical generalization error. This thesis aims to address this gap by conducting an empirical investigation into the impact of graph parameters on the generalization performance of GNNs.

To achieve this objective, two sets of experiments were conducted:

First, graph parameters were computed for a set of 49 datasets selected from the TUDataset~\cite{morris2020tudataset} In graph theory, each graph have its own unique properties, such as the average degree. In graph theory, graph parameters are used to quantify them. A dataset consist of multiple graphs, each with its graph parameter. For each dataset, the average values of the specified graph parameters were calculated across all graphs. In this thesis, 49 datasets from the TUDataset~\cite{morris2020tudataset} were used for the calculation.

Secondly, various GNN architectures were trained across multiple datasets, and it is revealed that their generalization abilities varied. The analysis identified a notable correlation between generalization capabilities and specific graph-theoretic parameters.

The thesis is organized as follows:
Chapter~\ref{chap:background} provides background information on graph neural networks and generalization. Chapter~\ref{chap:experiments} describes the methodology used to conduct the experiments. Chapter~\ref{chap:results} presents the experimental results and discusses the results and their implications. Finally, Chapter~\ref{chap:conclusions} concludes the thesis and discusses future work.

\section{Related work}
The subsequent section examines related work on GNNs and a focus on their generalization capabilities.

\subsection{Graph Neural Networks}
GNNs were initially proposed by Gori et al.~\cite{gori2005new} and Scarselli et al.~\cite{scarselli2008graph} as a variant of recurrent neural networks. These models demonstrated its significant generalization capabilities, marking a foundational step in the development of graph-based learning frameworks. The work of Kipf and Welling~\cite{kipf2016semi} introduced the Graph Convolutional Networks (GCN) model, which received widespread attention from the research community. Gilmer et al.~\cite{gilmer2017neural} introduced the Message Passing Neural Networks (MPNN) model, which demonstrated the significant results on the molecular property prediction benchmark. Velickovic et al.~\cite{velivckovic2017graph} proposed the Graph Attention Networks (GAT) model, which uses attention mechanisms to aggregate information from neighboring nodes. Brody et al.~\cite{brody2021attentive} proposed the GATv2 model, which outperforms the GAT model by introducing a dynamic graph attention variant.

\subsection{Generalization}
While significant progress has been made in understanding the generalization properties of Feed-forward Neural Networks (FNNs) and Recurrent Neural Networks (RNNs). The generalization capabilities of GNNs remain underexplored, primarily due to the challenges posed by the irregular and non-Euclidean nature of graph structures.

Golowich et al.~\cite{golowich2018size} showed the generalization abilities of FNNs with Rademacher complexity. The work of Zhang et al.~\cite{zhang2018stabilizing} used SVD-based parameterization to improve the generalization by effectively addressing gradient issues.

In terms of RNNs, Chen et al.~\cite{chen2019generalization} studied the generalization properties of vanilla RNNs as well as their variants. Z. Allen-Zhu et al.~\cite{allen2019can} demonstrates that vanilla SGD enables RNNs to efficiently learn concept classes and overcomes prior exponential generalization bounds. 

In contrast to the FNNs and RNNs, GNNs deal with irregular data structures. The related work on the generalization of GNNs is limited. Scarselli et al.~\cite{scarselli2018vapnik} explored the generalization ability of GNNs by using the Vapnik-Chervonenkis (VC) dimension and further indicated that the generalization ability of such models improve as the number of connected nodes. After that, Garg et al.~\cite{garg2020generalization} studied the generalization ability of GNNs via Rademacher bounds. In comparison to the work of Scarselli et al., Garg et al. provided a tighter bound on the generalization error of GNNs.

Nevertheless, Morris et al.~\cite{morris2024futuredirectionstheorygraph} pointed out that the generalization ability of GNNs is still not well understood. The bounds established in these studies typically rely solely on general graph parameters, such as the maximum degree or total number of nodes, while often overlooking more expressive graph parameters.