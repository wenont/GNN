\chapter{Introduction}\label{chap:introduction}
Graph learning became a hot topic in the field of machine learning in recent years. Graph Neural Networks (GNNs)~\cite{gori2005new}, also known as deep learning on graphs, are a class of neural networks that can operate on graph-structured data, such as social networks~\cite{easley2010networks}, biological networks~\cite{barabasi2004network}, and images~\cite{simonovsky2017dynamic}. GNNs use the structure of a graph to perform "message passing", allowing information to propagate across nodes and edges. The key idea is to iteratively aggregate and update node features by exchanging information with local neighboring nodes, which allows GNNs to capture the structural information of the graph. This information is then used to make predictions. GNNs have been demonstrated to be effective in a wide range of tasks, such as node classification~\cite{hamilton2017inductive}, edge prediction~\cite{morselli2021network}, and graph classification~\cite{gilmer2017neural}. Popular GNNs in recent years include Graph Convolutional Networks (GCN)~\cite{kipf2016semi}, Graph Attention Networks (GAT)~\cite{velickovic2020pointer}, and Message Passing Neural Networks (MPNN)~\cite{gilmer2017neural}.  

However, the behavior of GNNs is not well understood. Particularly, the relationship between graph parameters/characteristics and the empirical generalization error of GNNs is not clear.  The goal of this bachelor thesis is to understand empirically how graph parameters/characteristics impact the generalization error of GNNs. 

To reach this goal, I conduct two sets of experiments. First, I calculate the graph parameters for a set of datasets: Each graph has its own unique properties, such as the average degree. In graph theory, \textit{graph parameter} is used to quantify them. And a dataset consists of multiple graphs, each with its graph parameter. In the case, I calculate the average value of our desired graph parameters over all graphs in the dataset. In this thesis, 50 datasets from the TUDataset~\cite{morris2020tudataset} is used for the calculation. Second, I train different GNN models on each dataset and find that their generalization abilities varied. I show that there is a certain relationship between generalization ability and different graph parameters.

The thesis is organized as follows. In Chapter~\ref{chap:background}, I provide background information on graph neural networks and generalization. In Chapter~\ref{chap:experiments}, I describe the methodology used to conduct the experiments. In Chapter~\ref{chap:results}, I present the experimental results and discuss the results and their implications. Finally, in Chapter~\ref{chap:conclusions}, I conclude the thesis and discuss future work.


\section{Related work}
In the following section, I will discuss the related work on graph neural networks and generalization.

\subsection{Graph Neural Networks}
Graph neural networks were initially proposed by Gori et al.~\cite{gori2005new} and Scarselli et al.~\cite{scarselli2008graph} as a variant of recurrent neural networks and demonstrated its generalization capabilities. The work of Kipf and Welling~\cite{kipf2016semi} introduced the Graph Convolutional Networks (GCN) model, which received widespread attention from the research community. Velickovic et al.~\cite{velickovic2020pointer} proposed the Graph Attention Networks (GAT) model, which uses attention mechanisms to aggregate information from neighboring nodes. Brody et al.~\cite{brody2021attentive} proposed the GATv2 model, which improves the performance of the GAT model by introducing residual connections and multi-head attention. Gilmer et al.~\cite{gilmer2017neural} introduced the Message Passing Neural Networks (MPNN) model, which demonstrated the significant results on the molecular property prediction benchmark;

\subsection{Generalization}
While significant progress has been made in understanding the generalization properties of feed-forward neural networks (FNNs) and recurrent neural networks (RNNs), the generalization abilities of graph neural networks (GNNs) remain less explored due to their of irregular graph structures.

Golowich et al.~\cite{golowich2018size} showed the generalization abilities of Feed-forward neural networks (FNNs) with Rademacher complexity. The work of Zhang et al.~\cite{zhang2018stabilizing} used SVD-based parameterization to improve the generalization by effectively addressing gradient issues.

In terms of Recurrent neural networks(RNNs), Chen et al.~\cite{chen2019generalization} studied the generalization properties of vanilla RNNs as well as their variants. Z. Allen-Zhu et al.~\cite{allen2019can} demonstrates that vanilla SGD enables RNNs to efficiently learn concept classes and overcomes prior exponential generalization bounds. 

But unlike the FNNs and RNNs, GNNs deal with irregular data structures. The related work on the generalization of GNNs is limited. Scarselli et al.~\cite{scarselli2018vapnik} explored the generalization ability of GNNs by using the Vapnik-Chervonenkis (VC) dimension and further indicated that the generalization ability of such models improve as the number of connected nodes. After that, Garg et al.~\cite{garg2020generalization} studied the generalization ability of GNNs via Rademacher bounds. In comparison to the work of Scarselli et al., Garg et al. provided a tighter bound on the generalization error of GNNs.

Nevertheless, Morris et al.~\cite{morris2024futuredirectionstheorygraph} pointed out that the generalization ability of GNNs is still not well understood. The bounds established in these studies typically rely solely on general graph parameters, such as the maximum degree or total number of nodes, often overlooking more expressive graph parameters.